{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify sys.path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from platypus import NSGAII, ProcessPoolEvaluator, unique, nondominated\n",
    "import random\n",
    "import time\n",
    "\n",
    "import config.config as config\n",
    "from src.data_processing import read_arff, preprocess_data\n",
    "from src.utils import get_best_result_per_seed\n",
    "from src.evaluation import train_incremental_real_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATA_PATH = os.path.join('..', 'data', config.DATASET_NAME)\n",
    "\n",
    "dataset = read_arff(DATA_PATH)\n",
    "df_dict = preprocess_data(dataset)\n",
    "\n",
    "train_X_timeseries, train_Y_timeseries, val_X_timeseries, val_Y_timeseries, test_X_timeseries, test_Y_timeseries = df_dict['timeseries']\n",
    "train_X, train_Y, val_X, val_Y, test_X, test_Y = df_dict['normalized']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_function(algorithm):\n",
    "    solution_eval.append(algorithm.nfe)\n",
    "    nGen = len(solution_eval)\n",
    "\n",
    "    if nGen % config.FREC == 0 and nGen > 1:\n",
    "        incremental_X, incremental_Y = [], []\n",
    "        \n",
    "        for s in unique(nondominated(algorithm.result)):\n",
    "            incremental_X.append([int(v[0]) for v in s.variables])\n",
    "            incremental_Y.append(s.objectives[0])\n",
    "\n",
    "        # Convert 'Attributes' to a set of tuples for efficient lookup\n",
    "        global surrogate_dataset\n",
    "        existing_attributes = set(map(tuple, surrogate_dataset['Attributes']))\n",
    "\n",
    "        filtered_data = [\n",
    "            (combX, combY)\n",
    "            for combX, combY in zip(incremental_X, incremental_Y)\n",
    "            if tuple(combX) not in existing_attributes and np.any(combX[-config.N_STEPS:]) and np.any(combX)\n",
    "        ]\n",
    "\n",
    "        if filtered_data:\n",
    "            incremental_X_final, incremental_Y_final = zip(*filtered_data)\n",
    "            \n",
    "            new_entries = pd.DataFrame({\n",
    "                'Attributes': [np.array(x) for x in incremental_X_final],\n",
    "                'N atrib': [np.sum(x) for x in incremental_X_final],\n",
    "                'RMSE StepsAhead Train': None,\n",
    "                'MAE StepsAhead Train': None,\n",
    "                'CC StepsAhead Train': None,\n",
    "                'H Train': None,\n",
    "                'RMSE StepsAhead Val': None,\n",
    "                'MAE StepsAhead Val': None,\n",
    "                'CC StepsAhead Val': None,\n",
    "                'H Val': incremental_Y_final,\n",
    "                'RMSE StepsAhead Test': None,\n",
    "                'MAE StepsAhead Test': None,\n",
    "                'CC StepsAhead Test': None,\n",
    "                'H Test': None\n",
    "            })\n",
    "\n",
    "            # Efficiently update surrogate_dataset\n",
    "            surrogate_dataset = pd.concat([surrogate_dataset, new_entries], ignore_index=True)\n",
    "\n",
    "            datasets.append(surrogate_dataset)\n",
    "\n",
    "            # Train surrogate model\n",
    "            for _ in range(config.EPOCHS):\n",
    "                surrogate.partial_fit(incremental_X_final, incremental_Y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problems.AttributeSelection import AttributeSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAMES = ['Seed', 'Attributes', 'N selected', \n",
    "                'RMSE StepsAhead Train', 'MAE StepsAhead Train', 'CC StepsAhead Train', 'H Train',\n",
    "                'RMSE StepsAhead Val', 'MAE StepsAhead Val', 'CC StepsAhead Val', 'H Val', \n",
    "                'RMSE StepsAhead Test', 'MAE StepsAhead Test', 'CC StepsAhead Test', 'H Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generationsPerRun = []\n",
    "datasetsPerRun = []\n",
    "\n",
    "# NSGA-II\n",
    "if __name__ == \"__main__\":\n",
    "    dfSolutionsSGDR = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for seed in range(config.N_SEEDS):\n",
    "        # Load surrogate model\n",
    "        with open(f'../models/{config.DATASET_SAVE_NAME}-surrogate-SGDR-'+ str(seed) +'.pickle', 'rb') as f:\n",
    "            surrogate = pickle.load(f)[0]\n",
    "        # Load surrogate dataset\n",
    "        with open(f'../variables/{config.DATASET_SAVE_NAME}-surrogate-dataset.pickle', 'rb') as f:\n",
    "            surrogate_dataset = pickle.load(f)\n",
    "            \n",
    "        solution_eval = []\n",
    "        datasets = [surrogate_dataset] # to include de original dataset  \n",
    "        \n",
    "        problem = AttributeSelection(nVar=config.N_ATTRIB, nobjs=2, model=surrogate)\n",
    "        \n",
    "        print(\"--- Run %s ---\" % seed)\n",
    "        random.seed(seed)\n",
    "        with ProcessPoolEvaluator(config.N_JOBS) as evaluator:\n",
    "            algorithm = NSGAII(problem, evaluator=evaluator)\n",
    "            algorithm.run(config.N_EVAL, callback=callback_function)\n",
    "\n",
    "        generationsPerRun.append(solution_eval)\n",
    "        datasetsPerRun.append(datasets)\n",
    "            \n",
    "        results[str(seed)] = algorithm.result\n",
    "        df = train_incremental_real_models(unique(nondominated(algorithm.result)), \n",
    "                                   train_X, train_Y, val_X, val_Y, test_X, test_Y, seed)\n",
    "        dfSolutionsSGDR = pd.concat([dfSolutionsSGDR, df], ignore_index=True)\n",
    "            \n",
    "\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_result_per_seed(dfSolutionsSGDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-datasetsPerRun-SGDR-surrogateincremental.pickle', 'wb') as f:\n",
    "    pickle.dump([datasetsPerRun], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-dfSolutions-SGDR-surrogateincremental.pickle', 'wb') as f:\n",
    "    pickle.dump([dfSolutionsSGDR], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generationsPerRun = []\n",
    "datasetsPerRun = []\n",
    "\n",
    "# NSGA-II\n",
    "if __name__ == \"__main__\":\n",
    "    dfSolutionsMLP = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for seed in range(config.N_SEEDS):\n",
    "        # Load surrogate model\n",
    "        with open(f'../models/{config.DATASET_SAVE_NAME}-surrogate-MLP-'+ str(seed) +'.pickle', 'rb') as f:\n",
    "            surrogate = pickle.load(f)[0]\n",
    "        # Load surrogate dataset\n",
    "        with open(f'../variables/{config.DATASET_SAVE_NAME}-surrogate-dataset.pickle', 'rb') as f:\n",
    "            surrogate_dataset = pickle.load(f)\n",
    "            \n",
    "        solution_eval = []\n",
    "        datasets = [surrogate_dataset] # to include de original dataset  \n",
    "        \n",
    "        problem = AttributeSelection(nVar=config.N_ATTRIB, nobjs=2, model=surrogate)\n",
    "        \n",
    "        print(\"--- Run %s ---\" % seed)\n",
    "        random.seed(seed)\n",
    "        with ProcessPoolEvaluator(config.N_JOBS) as evaluator:\n",
    "            algorithm = NSGAII(problem, evaluator=evaluator)\n",
    "            algorithm.run(config.N_EVAL, callback=callback_function)\n",
    "\n",
    "        generationsPerRun.append(solution_eval)\n",
    "        datasetsPerRun.append(datasets)\n",
    "            \n",
    "        results[str(seed)] = algorithm.result\n",
    "        df = train_incremental_real_models(unique(nondominated(algorithm.result)), \n",
    "                                   train_X, train_Y, val_X, val_Y, test_X, test_Y, seed)\n",
    "        dfSolutionsMLP = pd.concat([dfSolutionsMLP, df], ignore_index=True)\n",
    "            \n",
    "\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_result_per_seed(dfSolutionsMLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-datasetsPerRun-MLP-surrogateincremental.pickle', 'wb') as f:\n",
    "    pickle.dump([datasetsPerRun], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-dfSolutions-MLP-surrogateincremental.pickle', 'wb') as f:\n",
    "    pickle.dump([dfSolutionsMLP], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
