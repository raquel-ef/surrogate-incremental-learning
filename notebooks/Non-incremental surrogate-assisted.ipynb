{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify sys.path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from platypus import NSGAII, ProcessPoolEvaluator, unique, nondominated\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "import random\n",
    "import time\n",
    "\n",
    "import config.config as config\n",
    "from src.data_processing import read_arff, preprocess_data\n",
    "from src.utils import get_best_result_per_seed\n",
    "from src.evaluation import create_surrogate_model_dataset, train_incremental_real_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "DATA_PATH = os.path.join('..', 'data', config.DATASET_NAME)\n",
    "\n",
    "dataset = read_arff(DATA_PATH)\n",
    "df_dict = preprocess_data(dataset)\n",
    "\n",
    "train_X_timeseries, train_Y_timeseries, val_X_timeseries, val_Y_timeseries, test_X_timeseries, test_Y_timeseries = df_dict['timeseries']\n",
    "train_X, train_Y, val_X, val_Y, test_X, test_Y = df_dict['normalized']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def callback_function(algorithm, model_type):\n",
    "    solution_eval.append(algorithm.nfe)\n",
    "    n_gen = len(solution_eval)\n",
    "\n",
    "    if n_gen % config.FREC == 0 and n_gen > 1:\n",
    "        incremental_X, incremental_Y = [], []\n",
    "\n",
    "        # Get new non-dominated solutions\n",
    "        for s in unique(nondominated(algorithm.result)):\n",
    "            incremental_X.append(np.array([int(v[0]) for v in s.variables], dtype=int))\n",
    "            incremental_Y.append(s.objectives[0])\n",
    "\n",
    "        global surrogate_dataset\n",
    "        listAtrib_set = {tuple(i) for i in surrogate_dataset['Attributes'].tolist()}  # Use set for fast lookup\n",
    "\n",
    "        new_entries = []\n",
    "        for combX, _ in zip(incremental_X, incremental_Y):\n",
    "            if tuple(combX) not in listAtrib_set and combX[-config.N_STEPS:].sum() != 0 and combX.sum() != 0:\n",
    "                new_entries.extend(create_surrogate_model_dataset(\n",
    "                    [np.asarray(combX)], train_X, train_Y, \n",
    "                    val_X, val_Y, test_X, test_Y\n",
    "                ).to_dict(orient='records'))\n",
    "\n",
    "        # Append new entries to the dataset\n",
    "        if new_entries:\n",
    "            surrogate_dataset = pd.concat([surrogate_dataset, pd.DataFrame(new_entries)], ignore_index=True)\n",
    "\n",
    "        datasets.append(surrogate_dataset)\n",
    "\n",
    "        # Train the surrogate model\n",
    "        surrogate_datasetX = np.stack(surrogate_dataset['Attributes'].to_numpy())\n",
    "        surrogate_datasetY = surrogate_dataset['H Val'].to_numpy()\n",
    "\n",
    "        if model_type == 'RandomForest':\n",
    "            surrogate = RandomForestRegressor(random_state=config.SEED_VALUE)\n",
    "        elif model_type == 'SGDRegressor':\n",
    "            surrogate = SGDRegressor(random_state=config.SEED_VALUE)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type. Choose from 'RandomForest' or 'SGDRegressor'\")\n",
    "\n",
    "        surrogate.fit(surrogate_datasetX, surrogate_datasetY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problems.AttributeSelection import AttributeSelection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMN_NAMES = ['Seed', 'Attributes', 'N selected', \n",
    "                'RMSE StepsAhead Train', 'MAE StepsAhead Train', 'CC StepsAhead Train', 'H Train',\n",
    "                'RMSE StepsAhead Val', 'MAE StepsAhead Val', 'CC StepsAhead Val', 'H Val', \n",
    "                'RMSE StepsAhead Test', 'MAE StepsAhead Test', 'CC StepsAhead Test', 'H Test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generationsPerRun = []\n",
    "datasetsPerRun = []\n",
    "\n",
    "# NSGA-II\n",
    "if __name__ == \"__main__\":\n",
    "    dfSolutionsRF = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for seed in range(config.N_SEEDS):\n",
    "        # Load surrogate model\n",
    "        with open(f'../models/{config.DATASET_SAVE_NAME}-surrogate-RF-'+ str(seed) +'.pickle', 'rb') as f:\n",
    "            surrogate = pickle.load(f)[0]\n",
    "        # Load surrogate dataset\n",
    "        with open(f'../variables/{config.DATASET_SAVE_NAME}-surrogate-dataset.pickle', 'rb') as f:\n",
    "            surrogate_dataset = pickle.load(f)\n",
    "            \n",
    "        solution_eval = []\n",
    "        datasets = [surrogate_dataset] # to include de original dataset  \n",
    "        \n",
    "        problem = AttributeSelection(nVar=config.N_ATTRIB, nobjs=2, model=surrogate)\n",
    "        \n",
    "        print(\"--- Run %s ---\" % seed)\n",
    "        random.seed(seed)\n",
    "        with ProcessPoolEvaluator(config.N_JOBS) as evaluator:\n",
    "            algorithm = NSGAII(problem, evaluator=evaluator)\n",
    "            algorithm.run(config.N_EVAL, callback=lambda alg: callback_function(alg, 'RandomForest'))\n",
    "\n",
    "        generationsPerRun.append(solution_eval)\n",
    "        datasetsPerRun.append(datasets)\n",
    "            \n",
    "        results[str(seed)] = algorithm.result\n",
    "        df = train_incremental_real_models(unique(nondominated(algorithm.result)), \n",
    "                                   train_X, train_Y, val_X, val_Y, test_X, test_Y, seed)\n",
    "        dfSolutionsRF = pd.concat([dfSolutionsRF, df], ignore_index=True)\n",
    "            \n",
    "\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_best_result_per_seed(dfSolutionsRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-datasetsPerRun-RF-datsetincremental.pickle', 'wb') as f:\n",
    "    pickle.dump([datasetsPerRun], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-dfSolutions-RF-datsetincremental.pickle', 'wb') as f:\n",
    "    pickle.dump([dfSolutionsRF], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generationsPerRun = []\n",
    "datasetsPerRun = []\n",
    "\n",
    "# NSGA-II\n",
    "if __name__ == \"__main__\":\n",
    "    dfSolutionsSGDR = pd.DataFrame(columns=COLUMN_NAMES)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for seed in range(config.N_SEEDS):\n",
    "        # Load surrogate model\n",
    "        with open(f'../models/{config.DATASET_SAVE_NAME}-surrogate-SGDR-'+ str(seed) +'.pickle', 'rb') as f:\n",
    "            surrogate = pickle.load(f)[0]\n",
    "        # Load surrogate dataset\n",
    "        with open(f'../variables/{config.DATASET_SAVE_NAME}-surrogate-dataset.pickle', 'rb') as f:\n",
    "            surrogate_dataset = pickle.load(f)\n",
    "            \n",
    "        solution_eval = []\n",
    "        datasets = [surrogate_dataset] # to include de original dataset  \n",
    "        \n",
    "        problem = AttributeSelection(nVar=config.N_ATTRIB, nobjs=2, model=surrogate)\n",
    "        \n",
    "        print(\"--- Run %s ---\" % seed)\n",
    "        random.seed(seed)\n",
    "        with ProcessPoolEvaluator(config.N_JOBS) as evaluator:\n",
    "            algorithm = NSGAII(problem, evaluator=evaluator)\n",
    "            algorithm.run(config.N_EVAL, callback=lambda alg: callback_function(alg, 'SGDRegressor'))\n",
    "\n",
    "        generationsPerRun.append(solution_eval)\n",
    "        datasetsPerRun.append(datasets)\n",
    "            \n",
    "        results[str(seed)] = algorithm.result\n",
    "        df = train_incremental_real_models(unique(nondominated(algorithm.result)), \n",
    "                                   train_X, train_Y, val_X, val_Y, test_X, test_Y, seed)\n",
    "        dfSolutionsSGDR = pd.concat([dfSolutionsSGDR, df], ignore_index=True)\n",
    "            \n",
    "\n",
    "    print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_best_result_per_seed(dfSolutionsSGDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-datasetsPerRun-SGDR-datsetincremental.pickle', 'wb') as f:\n",
    "    pickle.dump([datasetsPerRun], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../variables/{config.DATASET_SAVE_NAME}-dfSolutions-SGDR-datsetincremental.pickle', 'wb') as f:\n",
    "    pickle.dump([dfSolutionsSGDR], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
